import os
import logging
from pathlib import Path
from typing import List
import asyncio

from fastmcp import FastMCP
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    Settings,
    Settings,
    load_index_from_storage,
)
from llama_index.core.schema import TextNode
from llama_index.core.node_parser import CodeSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
import re
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.core import QueryBundle

# è¿½åŠ : ãƒ­ã‚°ã‚’ stderr ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºã™è¨­å®šï¼ˆstdio ã‚’æ±šæŸ“ã—ãªã„ã‚ˆã†ã«ï¼‰
import os
import sys
import logging
from logging.handlers import RotatingFileHandler

# server.py ã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å…ˆã‚’ Desktop ã«å›ºå®šï¼‰
import os
import sys
import logging
from logging.handlers import RotatingFileHandler

# ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ç›´ä¸‹ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ã‚’ãƒ«ãƒ¼ãƒˆã«ã™ã‚‹
REPO_ROOT = Path(__file__).resolve().parent
# ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
logger = logging.getLogger("mutagen-rag")
LOG_FILE = os.getenv("MCP_LOG_FILE", str(REPO_ROOT / "mcp_server.log"))

def setup_logging_desktop():
    formatter = logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s")
    stderr_handler = logging.StreamHandler(stream=sys.stderr)
    stderr_handler.setFormatter(formatter)
    stderr_handler.setLevel(logging.INFO)
   
    root = logging.getLogger()
    for h in root.handlers[:]:
        root.removeHandler(h)
    root.addHandler(stderr_handler)
    root.setLevel(logging.INFO)

    try:
        os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
        # æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆ
        with open(LOG_FILE, "a", encoding="utf-8"):
            pass
        file_handler = RotatingFileHandler(LOG_FILE, maxBytes=2 * 1024 * 1024, backupCount=3, encoding="utf-8")
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.INFO)
        root.addHandler(file_handler)
        logging.getLogger(__name__).info("Logging initialized to %s", LOG_FILE)
    except Exception as e:
        logging.getLogger(__name__).warning("Cannot open log file %s: %s. Continuing with stderr only.", LOG_FILE, e)

setup_logging_desktop()

# Configuration
# Code splitter for C# - chunk by semantic code blocks rather than tokens
# Falls back to default splitting if tree-sitter is unavailable
try:
    from llama_index.core.node_parser import CodeSplitter
    code_splitter = CodeSplitter(
        language="c_sharp",
        chunk_lines=40,  # Roughly 40 lines per chunk
        chunk_lines_overlap=15,  # Overlap to maintain context
        max_chars=2048,  # Safety limit
    )
    transformations_list = [code_splitter]
    logger.info("CodeSplitter (C#) initialized successfully.")
except ImportError as e:
    logger.warning(f"Tree-sitter or CodeSplitter not available: {e}. Falling back to default splitting.")
    transformations_list = []  # Use default LlamaIndex splitting

# Get Mutagen repository path from environment variable or default
MUTAGEN_REPO_PATH = os.getenv(
    "MUTAGEN_REPO_PATH",
    "./Mutagen/Mutagen.Bethesda.Core"
)
STORAGE_PATH = "./storage"
COLLECTION_NAME = "mutagen_handwritten_code"

# BM25 Retriever Cache (cleared on refresh_index)
_CACHED_BM25_RETRIEVER = None

# Initialize FastMCP
# Dependencies list is for metadata, actual imports are handled by python environment
mcp = FastMCP("Mutagen Helper", dependencies=["fastmcp", "llama-index", "chromadb"])

# Initialize Embedding Model
# Using a small, efficient model suitable for local use
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

# Initialize ChromaDB Client (Persistent)
chroma_client = chromadb.PersistentClient(path=STORAGE_PATH)

# Dynamic batch size calculation
try:
    # ChromaDB 0.4+ might have max_batch_size
    max_batch = getattr(chroma_client, "max_batch_size", 5000)
    SAFE_BATCH_SIZE = min(5000, max_batch // 2)
except Exception:
    SAFE_BATCH_SIZE = 2000 # Fallback

logger.info(f"ChromaDB max_batch_size: {getattr(chroma_client, 'max_batch_size', 'Unknown')}")
logger.info(f"Using batch size: {SAFE_BATCH_SIZE}")

def is_generated_file_fast(file_path: Path) -> bool:
    """
    Fast path-based filtering without file I/O.
    Checks file extensions and directory names only.
    """
    path_str = str(file_path)
    
    # 1. File name patterns
    generated_suffixes = [".g.cs", ".Autogen.cs", ".Generated.cs"]
    if any(path_str.endswith(suffix) for suffix in generated_suffixes):
        return True
    
    # 2. Directory names
    excluded_dirs = ["Generated", "obj", "bin", ".vs", "Autogenerated"]
    if any(excluded_dir in file_path.parts for excluded_dir in excluded_dirs):
        return True
        
    return False

def is_header_generated(text: str) -> bool:
    """
    Check if file content has auto-generated markers.
    This is the slow check that reads file content.
    """
    # Only check first 1000 chars for performance
    header = text[:1000]
    generated_markers = [
        "<auto-generated>",
        "This code was generated",
        "// <auto-generated />",
        "Auto-generated code"
    ]
    return any(marker in header for marker in generated_markers)

def extract_metadata(file_path: str, content: str) -> dict:
    """
    Extracts C# specific metadata (namespace, class, etc.)
    """
    metadata = {}
    
    # Extract Namespace
    namespace_match = re.search(r'namespace\s+([\w\.]+)', content)
    if namespace_match:
        metadata["namespace"] = namespace_match.group(1)
        
    # Extract Class/Interface/Struct names
    # This is a simplified regex and might capture multiple if defined in one file
    types = []
    for match in re.finditer(r'(class|interface|struct|enum|record)\s+([\w]+)', content):
        types.append(f"{match.group(1)}:{match.group(2)}")
    
    if types:
        # Limit the number of types to avoid metadata overflow
        joined_types = ", ".join(types)
        if len(joined_types) > 500:
            joined_types = joined_types[:497] + "..."
        metadata["defined_types"] = joined_types
        
    return metadata

@mcp.tool()
def refresh_index(repo_path: str = MUTAGEN_REPO_PATH) -> str:
    """
    Scans the Mutagen repository and rebuilds the vector index.
    Filters out auto-generated files.
    
    Args:
        repo_path: Path to the Mutagen src directory.
    """
    import time
    global _CACHED_BM25_RETRIEVER
    
    start_time = time.time()
    
    # Clear BM25 cache on refresh
    _CACHED_BM25_RETRIEVER = None
    logger.info("Cleared BM25 retriever cache")
    
    repo_path_obj = Path(repo_path)
    if not repo_path_obj.exists():
        return f"Error: Repository path does not exist: {repo_path}"

    logger.info(f"Scanning repository at: {repo_path}")
    
    # PRE-FILTER FILES: Build filtered file list BEFORE loading into memory
    # This dramatically reduces memory usage and I/O for large codebases
    all_files = []
    excluded_dirs = {"obj", "bin", "Generated", ".vs", ".git", "Autogenerated"}
    
    for root, dirs, files in os.walk(str(repo_path_obj)):
        # Modify dirs in-place to skip excluded directories (prevents descending into them)
        dirs[:] = [d for d in dirs if d not in excluded_dirs]
        
        for file in files:
            if file.endswith(".cs"):
                full_path = Path(root) / file
                # Fast path-based filtering (no file I/O)
                if not is_generated_file_fast(full_path):
                    all_files.append(str(full_path))
    
    logger.info(f"Pre-filtered to {len(all_files)} .cs files before loading")
    
    if not all_files:
        return f"Error: No .cs files found after pre-filtering in {repo_path}"
    
    # Load only the pre-filtered files
    reader = SimpleDirectoryReader(
        input_files=all_files,
        filename_as_id=True
    )
    
    try:
        all_docs = reader.load_data()
    except Exception as e:
        return f"Error loading data: {e}"
    
    logger.info(f"Loaded {len(all_docs)} documents. Applying header-based filtering...")
    
    # Filter by content (header check) - only for docs that passed path filter
    filtered_docs = [
        d for d in all_docs 
        if not is_header_generated(d.text)
    ]
    
    excluded_count = len(all_docs) - len(filtered_docs)
    logger.info(f"Filtered down to {len(filtered_docs)} documents. Excluded {excluded_count} files with generated headers.")
    
    if not filtered_docs:
        return "Error: No documents remained after filtering. Check path and filter logic."

    # Add metadata
    for doc in filtered_docs:
        doc.metadata["source"] = "mutagen_handwritten"
        doc.metadata["indexed_at"] = str(Path(doc.metadata["file_path"]).stat().st_mtime)
        # Extract additional C# metadata
        try:
            doc.metadata.update(extract_metadata(doc.metadata["file_path"], doc.text))
        except Exception as e:
            logger.warning(f"Failed to extract metadata for {doc.metadata['file_path']}: {e}")

    # Initialize ChromaDB Collection
    chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    # Create Index with CodeSplitter for C#-aware chunking
    # insert_batch_size=SAFE_BATCH_SIZE to stay under ChromaDB limit
    index = VectorStoreIndex.from_documents(
        filtered_docs, 
        storage_context=storage_context,
        embed_model=embed_model,
        transformations=[code_splitter],  # Use C# code-aware chunking
        show_progress=True,
        insert_batch_size=SAFE_BATCH_SIZE
    )
    
    # Persist to disk (LlamaIndex metadata)
    index.storage_context.persist(persist_dir=STORAGE_PATH)
    
    elapsed = time.time() - start_time
    return f"âœ… Index refresh complete.\n- Time taken: {elapsed:.2f}s\n- Handwritten files registered: {len(filtered_docs)}\n- Excluded generated files: {len(all_files) - len(filtered_docs) + excluded_count}\n- Storage path: {STORAGE_PATH}"
@mcp.tool()
def search_repository(query: str, top_k: int = 5) -> str:
    """
    Searches the Mutagen repository index for relevant code snippets.
    
    Args:
        query: The search query.
        top_k: Number of results to return.
    """
    try:
        # Check if collection exists
        try:
            chroma_collection = chroma_client.get_collection(COLLECTION_NAME)
        except ValueError:
             return "âŒ Index does not exist. Please run 'refresh_index' first."

        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            persist_dir=STORAGE_PATH
        )
        
        index = load_index_from_storage(
            storage_context,
            embed_model=embed_model
        )
        
        # Query Engine
        # Using "refine" as requested for better code retrieval accuracy
        # Query Engine Construction with Hybrid Search and Reranking
        
        # 1. BM25 Retriever (Sparse) - with caching for performance
        retriever_bm25 = get_bm25_retriever(index, chroma_collection, top_k)

        # 2. Vector Retriever (Dense)
        retriever_vector = index.as_retriever(
            similarity_top_k=top_k * 3
        )
        
        # 3. Hybrid Fusion & Query Engine
        reranker = None
        try:
            final_retriever = retriever_vector # Default to vector if hybrid fails
            
            if retriever_bm25:
                # Combines results from both retrievers
                # Note: This might fail if LLM is missing (default OpenAI)
                retriever_fusion = QueryFusionRetriever(
                    [retriever_vector, retriever_bm25],
                    similarity_top_k=top_k * 2, # Candidates for reranking
                    num_queries=1,
                    mode="reciprocal_rank",
                    use_async=False,
                    verbose=True
                )
                final_retriever = retriever_fusion
            else:
                logger.warning("Falling back to Vector Search only (BM25 missing).")
            
            # 4. Reranker
            # Re-scores the fused results to find the absolute best matches
            reranker = SentenceTransformerRerank(
                model="BAAI/bge-reranker-base",
                top_n=top_k
            )

            # 5. Query Engine
            query_engine = index.as_query_engine(
                retriever=final_retriever,
                node_postprocessors=[reranker],
                response_mode="refine" 
            )
            response = query_engine.query(query)
            response_text = str(response)
            source_nodes = response.source_nodes
        except Exception as e:
            # Fallback if LLM is not configured
            logger.warning(f"Query failed (likely missing LLM): {e}. Returning retrieval results only.")
            # Use vector retriever directly as safe fallback to avoid QueryFusionRetriever LLM dependency
            source_nodes = retriever_vector.retrieve(query)
            # Apply reranker manually if we fell back to retriever
            if reranker:
                source_nodes = reranker.postprocess_nodes(source_nodes, query_bundle=QueryBundle(query))
            response_text = "âš ï¸ LLM not configured or failed. Showing retrieved documents only."
        
        # Append source files with more details
        sources_list = []
        for node in source_nodes:
            # Handle NodeWithScore or TextNode
            # If it's NodeWithScore, get node
            n = node.node if hasattr(node, 'node') else node
            score = f"{node.score:.4f}" if hasattr(node, 'score') and node.score is not None else "N/A"
            
            file_path = n.metadata.get('file_path', 'Unknown')
            types = n.metadata.get('defined_types', '')
            sources_list.append(f"- {file_path} (Score: {score}) {f'[{types}]' if types else ''}")
            
        sources = "\n".join(sources_list)
        
        return f"{response_text}\n\nðŸ“‚ Source Files:\n{sources}"
        
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return f"âŒ Error during search: {e}"

@mcp.tool()
def get_index_stats() -> str:
    """Returns statistics about the current index."""
    try:
        chroma_collection = chroma_client.get_collection(COLLECTION_NAME)
        count = chroma_collection.count()
        return f"ðŸ“Š Index Statistics\n- Total Documents: {count}\n- Collection Name: {COLLECTION_NAME}\n- Storage Path: {STORAGE_PATH}"
    except Exception as e:
        return f"Failed to get stats: {str(e)}"

if __name__ == "__main__":
    try:
        mcp.run()
    except asyncio.CancelledError:
        logger.info("Shutdown requested (CancelledError). Exiting cleanly.")
    except KeyboardInterrupt:
        logger.info("KeyboardInterrupt received, shutting down MCP server.")
    except Exception as e:
        logger.exception("MCP server terminated with exception: %s", e)
