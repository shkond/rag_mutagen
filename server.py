import os
import logging
from pathlib import Path
from typing import List
import asyncio

from fastmcp import FastMCP
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    Settings,
    Settings,
    load_index_from_storage,
)
from llama_index.core.schema import TextNode
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
import re
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.core import QueryBundle

# ËøΩÂä†: „É≠„Ç∞„Çí stderr „Å®„Éï„Ç°„Ç§„É´„Å´Âá∫„ÅôË®≠ÂÆöÔºàstdio „ÇíÊ±öÊüì„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´Ôºâ
import os
import sys
import logging
from logging.handlers import RotatingFileHandler

# server.py „Å´ËøΩÂä†Ôºà„Éï„Ç°„Ç§„É´ÂÖà„Çí Desktop „Å´Âõ∫ÂÆöÔºâ
import os
import sys
import logging
from logging.handlers import RotatingFileHandler
 # ‰æã: server.py „ÅÆ logging ÂàùÊúüÂåñ„ÅÆ„Åô„ÅêÂæå„Å´ËøΩÂä†
logger = logging.getLogger("mutagen-rag")


def setup_logging_desktop():
    formatter = logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s")
    stderr_handler = logging.StreamHandler(stream=sys.stderr)
    stderr_handler.setFormatter(formatter)
    stderr_handler.setLevel(logging.INFO)
   
    root = logging.getLogger()
    for h in root.handlers[:]:
        root.removeHandler(h)
    root.addHandler(stderr_handler)
    root.setLevel(logging.INFO)

    try:
        os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
        # Êõ∏„ÅçËæº„Åø„ÉÜ„Çπ„Éà
        with open(LOG_FILE, "a", encoding="utf-8"):
            pass
        file_handler = RotatingFileHandler(LOG_FILE, maxBytes=2 * 1024 * 1024, backupCount=3, encoding="utf-8")
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.INFO)
        root.addHandler(file_handler)
        logging.getLogger(__name__).info("Logging initialized to %s", LOG_FILE)
    except Exception as e:
        logging.getLogger(__name__).warning("Cannot open log file %s: %s. Continuing with stderr only.", LOG_FILE, e)

setup_logging_desktop()

# Configuration
# Set chunk size to 2048 and overlap to 200 for better code context
Settings.chunk_size = 2048
Settings.chunk_overlap = 200

# Get Mutagen repository path from environment variable or default
MUTAGEN_REPO_PATH = os.getenv(
    "MUTAGEN_REPO_PATH",
    "./Mutagen/Mutagen.Bethesda.Core"
)
STORAGE_PATH = "./storage"
COLLECTION_NAME = "mutagen_handwritten_code"

# Initialize FastMCP
# Dependencies list is for metadata, actual imports are handled by python environment
mcp = FastMCP("Mutagen Helper", dependencies=["fastmcp", "llama-index", "chromadb"])

# Initialize Embedding Model
# Using a small, efficient model suitable for local use
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

# Initialize ChromaDB Client (Persistent)
chroma_client = chromadb.PersistentClient(path=STORAGE_PATH)

# Dynamic batch size calculation
try:
    # ChromaDB 0.4+ might have max_batch_size
    max_batch = getattr(chroma_client, "max_batch_size", 5000)
    SAFE_BATCH_SIZE = min(5000, max_batch // 2)
except Exception:
    SAFE_BATCH_SIZE = 2000 # Fallback

logger.info(f"ChromaDB max_batch_size: {getattr(chroma_client, 'max_batch_size', 'Unknown')}")
logger.info(f"Using batch size: {SAFE_BATCH_SIZE}")

def is_generated_file(file_path: str) -> bool:
    """
    Detects if a file is auto-generated by Mutagen.
    Checks for:
    1. File extensions (.g.cs, .Autogen.cs, .Generated.cs)
    2. Directory names (Generated, obj, bin, .vs, Autogenerated)
    3. Header content (<auto-generated>, etc.)
    """
    path_obj = Path(file_path)
    path_str = str(file_path)
    
    # 1. File name patterns
    generated_suffixes = [".g.cs", ".Autogen.cs", ".Generated.cs"]
    if any(path_str.endswith(suffix) for suffix in generated_suffixes):
        return True
    
    # 2. Directory names
    excluded_dirs = ["Generated", "obj", "bin", ".vs", "Autogenerated"]
    if any(excluded_dir in path_obj.parts for excluded_dir in excluded_dirs):
        return True
    
    # 3. Header content
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            # Read the first 1000 characters to check for the header
            header = f.read(1000)
            generated_markers = [
                "<auto-generated>",
                "This code was generated",
                "// <auto-generated />",
                "Auto-generated code"
            ]
            if any(marker in header for marker in generated_markers):
                return True
    except Exception as e:
        # Skip binary files or files that cannot be read
        logger.warning(f"Could not read file {file_path}: {e}")
        return True # Fail safe: exclude if unreadable
        
    return False

def extract_metadata(file_path: str, content: str) -> dict:
    """
    Extracts C# specific metadata (namespace, class, etc.)
    """
    metadata = {}
    
    # Extract Namespace
    namespace_match = re.search(r'namespace\s+([\w\.]+)', content)
    if namespace_match:
        metadata["namespace"] = namespace_match.group(1)
        
    # Extract Class/Interface/Struct names
    # This is a simplified regex and might capture multiple if defined in one file
    types = []
    for match in re.finditer(r'(class|interface|struct|enum|record)\s+([\w]+)', content):
        types.append(f"{match.group(1)}:{match.group(2)}")
    
    if types:
        # Limit the number of types to avoid metadata overflow
        joined_types = ", ".join(types)
        if len(joined_types) > 500:
            joined_types = joined_types[:497] + "..."
        metadata["defined_types"] = joined_types
        
    return metadata

@mcp.tool()
def refresh_index(repo_path: str = MUTAGEN_REPO_PATH) -> str:
    """
    Scans the Mutagen repository and rebuilds the vector index.
    Filters out auto-generated files.
    
    Args:
        repo_path: Path to the Mutagen src directory.
    """
    import time
    start_time = time.time()
    
    repo_path_obj = Path(repo_path)
    if not repo_path_obj.exists():
        return f"Error: Repository path does not exist: {repo_path}"

    logger.info(f"Scanning repository at: {repo_path}")
    
    # Load data
    reader = SimpleDirectoryReader(
        input_dir=str(repo_path_obj),
        recursive=True,
        required_exts=[".cs"],
        filename_as_id=True
    )
    
    try:
        all_docs = reader.load_data()
    except Exception as e:
        return f"Error loading data: {e}"
    
    logger.info(f"Loaded {len(all_docs)} documents. Filtering generated files...")
    
    # Filter out generated files
    filtered_docs = [
        d for d in all_docs 
        if not is_generated_file(d.metadata.get("file_path", ""))
    ]
    
    excluded_count = len(all_docs) - len(filtered_docs)
    logger.info(f"Filtered down to {len(filtered_docs)} documents. Excluded {excluded_count} generated files.")
    
    if not filtered_docs:
        return "Error: No documents remained after filtering. Check path and filter logic."

    # Add metadata
    for doc in filtered_docs:
        doc.metadata["source"] = "mutagen_handwritten"
        doc.metadata["indexed_at"] = str(Path(doc.metadata["file_path"]).stat().st_mtime)
        # Extract additional C# metadata
        try:
            doc.metadata.update(extract_metadata(doc.metadata["file_path"], doc.text))
        except Exception as e:
            logger.warning(f"Failed to extract metadata for {doc.metadata['file_path']}: {e}")

    # Initialize ChromaDB Collection
    chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    # Create Index
    # insert_batch_size=SAFE_BATCH_SIZE to stay under ChromaDB limit
    index = VectorStoreIndex.from_documents(
        filtered_docs, 
        storage_context=storage_context,
        embed_model=embed_model,
        show_progress=True,
        insert_batch_size=SAFE_BATCH_SIZE
    )
    
    # Persist to disk (LlamaIndex metadata)
    index.storage_context.persist(persist_dir=STORAGE_PATH)
    
    elapsed = time.time() - start_time
    return f"‚úÖ Index refresh complete.\n- Time taken: {elapsed:.2f}s\n- Handwritten files registered: {len(filtered_docs)}\n- Excluded generated files: {excluded_count}\n- Storage path: {STORAGE_PATH}"

@mcp.tool()
def search_repository(query: str, top_k: int = 5) -> str:
    """
    Searches the Mutagen repository index for relevant code snippets.
    
    Args:
        query: The search query.
        top_k: Number of results to return.
    """
    try:
        # Check if collection exists
        try:
            chroma_collection = chroma_client.get_collection(COLLECTION_NAME)
        except ValueError:
             return "‚ùå Index does not exist. Please run 'refresh_index' first."

        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            persist_dir=STORAGE_PATH
        )
        
        index = load_index_from_storage(
            storage_context,
            embed_model=embed_model
        )
        
        # Query Engine
        # Using "refine" as requested for better code retrieval accuracy
        # Query Engine Construction with Hybrid Search and Reranking
        
        # 1. BM25 Retriever (Sparse)
        # We construct it from the docstore of the loaded index
        # Note: This builds the BM25 index in memory, which is fast for <5k files
        retriever_bm25 = None
        try:
            nodes = list(index.docstore.docs.values())
            if not nodes:
                logger.info("Docstore empty, fetching nodes from ChromaDB for BM25...")
                data = chroma_collection.get()
                if data and data['documents']:
                    for i, text in enumerate(data['documents']):
                        # Reconstruct TextNode
                        # Note: We might lose some node attributes not stored in metadata/text
                        # but it's enough for BM25
                        node = TextNode(
                            text=text, 
                            id_=data['ids'][i], 
                            metadata=data['metadatas'][i] if data['metadatas'] else {}
                        )
                        nodes.append(node)
            
            logger.info(f"Building BM25 index from {len(nodes)} nodes...")
            if nodes:
                retriever_bm25 = BM25Retriever.from_defaults(
                    nodes=nodes,
                    similarity_top_k=top_k * 3 # Fetch more candidates for fusion
                )
            else:
                logger.warning("No nodes found for BM25.")
        except Exception as e:
            logger.error(f"Failed to initialize BM25Retriever: {e}")

        # 2. Vector Retriever (Dense)
        retriever_vector = index.as_retriever(
            similarity_top_k=top_k * 3
        )
        
        # 3. Hybrid Fusion & Query Engine
        reranker = None
        try:
            final_retriever = retriever_vector # Default to vector if hybrid fails
            
            if retriever_bm25:
                # Combines results from both retrievers
                # Note: This might fail if LLM is missing (default OpenAI)
                retriever_fusion = QueryFusionRetriever(
                    [retriever_vector, retriever_bm25],
                    similarity_top_k=top_k * 2, # Candidates for reranking
                    num_queries=1,
                    mode="reciprocal_rank",
                    use_async=False,
                    verbose=True
                )
                final_retriever = retriever_fusion
            else:
                logger.warning("Falling back to Vector Search only (BM25 missing).")
            
            # 4. Reranker
            # Re-scores the fused results to find the absolute best matches
            reranker = SentenceTransformerRerank(
                model="BAAI/bge-reranker-base",
                top_n=top_k
            )

            # 5. Query Engine
            query_engine = index.as_query_engine(
                retriever=final_retriever,
                node_postprocessors=[reranker],
                response_mode="refine" 
            )
            response = query_engine.query(query)
            response_text = str(response)
            source_nodes = response.source_nodes
        except Exception as e:
            # Fallback if LLM is not configured
            logger.warning(f"Query failed (likely missing LLM): {e}. Returning retrieval results only.")
            # Use vector retriever directly as safe fallback to avoid QueryFusionRetriever LLM dependency
            source_nodes = retriever_vector.retrieve(query)
            # Apply reranker manually if we fell back to retriever
            if reranker:
                source_nodes = reranker.postprocess_nodes(source_nodes, query_bundle=QueryBundle(query))
            response_text = "‚ö†Ô∏è LLM not configured or failed. Showing retrieved documents only."
        
        # Append source files with more details
        sources_list = []
        for node in source_nodes:
            # Handle NodeWithScore or TextNode
            # If it's NodeWithScore, get node
            n = node.node if hasattr(node, 'node') else node
            score = f"{node.score:.4f}" if hasattr(node, 'score') and node.score is not None else "N/A"
            
            file_path = n.metadata.get('file_path', 'Unknown')
            types = n.metadata.get('defined_types', '')
            sources_list.append(f"- {file_path} (Score: {score}) {f'[{types}]' if types else ''}")
            
        sources = "\n".join(sources_list)
        
        return f"{response_text}\n\nüìÇ Source Files:\n{sources}"
        
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return f"‚ùå Error during search: {e}"

@mcp.tool()
def get_index_stats() -> str:
    """Returns statistics about the current index."""
    try:
        chroma_collection = chroma_client.get_collection(COLLECTION_NAME)
        count = chroma_collection.count()
        return f"üìä Index Statistics\n- Total Documents: {count}\n- Collection Name: {COLLECTION_NAME}\n- Storage Path: {STORAGE_PATH}"
    except Exception as e:
        return f"Failed to get stats: {str(e)}"

if __name__ == "__main__":
    try:
        mcp.run()
    except asyncio.CancelledError:
        logger.info("Shutdown requested (CancelledError). Exiting cleanly.")
    except KeyboardInterrupt:
        logger.info("KeyboardInterrupt received, shutting down MCP server.")
    except Exception as e:
        logger.exception("MCP server terminated with exception: %s", e)
