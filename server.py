import os
import logging
from pathlib import Path
from typing import List

from fastmcp import FastMCP
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    Settings,
    load_index_from_storage,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("mutagen-rag")

# Configuration
# Set chunk size to 512 to avoid ChromaDB batch size limits (max 5461)
Settings.chunk_size = 512

# Get Mutagen repository path from environment variable or default
MUTAGEN_REPO_PATH = os.getenv(
    "MUTAGEN_REPO_PATH",
    "./Mutagen/Mutagen.Bethesda.Core"
)
STORAGE_PATH = "./storage"
COLLECTION_NAME = "mutagen_handwritten_code"

# Initialize FastMCP
# Dependencies list is for metadata, actual imports are handled by python environment
mcp = FastMCP("Mutagen Helper", dependencies=["fastmcp", "llama-index", "chromadb"])

# Initialize Embedding Model
# Using a small, efficient model suitable for local use
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

# Initialize ChromaDB Client (Persistent)
chroma_client = chromadb.PersistentClient(path=STORAGE_PATH)

# Dynamic batch size calculation
try:
    # ChromaDB 0.4+ might have max_batch_size
    max_batch = getattr(chroma_client, "max_batch_size", 5000)
    SAFE_BATCH_SIZE = min(5000, max_batch // 2)
except Exception:
    SAFE_BATCH_SIZE = 2000 # Fallback

logger.info(f"ChromaDB max_batch_size: {getattr(chroma_client, 'max_batch_size', 'Unknown')}")
logger.info(f"Using batch size: {SAFE_BATCH_SIZE}")

def is_generated_file(file_path: str) -> bool:
    """
    Detects if a file is auto-generated by Mutagen.
    Checks for:
    1. File extensions (.g.cs, .Autogen.cs, .Generated.cs)
    2. Directory names (Generated, obj, bin, .vs, Autogenerated)
    3. Header content (<auto-generated>, etc.)
    """
    path_obj = Path(file_path)
    path_str = str(file_path)
    
    # 1. File name patterns
    generated_suffixes = [".g.cs", ".Autogen.cs", ".Generated.cs"]
    if any(path_str.endswith(suffix) for suffix in generated_suffixes):
        return True
    
    # 2. Directory names
    excluded_dirs = ["Generated", "obj", "bin", ".vs", "Autogenerated"]
    if any(excluded_dir in path_obj.parts for excluded_dir in excluded_dirs):
        return True
    
    # 3. Header content
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            # Read the first 1000 characters to check for the header
            header = f.read(1000)
            generated_markers = [
                "<auto-generated>",
                "This code was generated",
                "// <auto-generated />",
                "Auto-generated code"
            ]
            if any(marker in header for marker in generated_markers):
                return True
    except Exception as e:
        # Skip binary files or files that cannot be read
        logger.warning(f"Could not read file {file_path}: {e}")
        return True # Fail safe: exclude if unreadable
        
    return False

@mcp.tool()
def refresh_index(repo_path: str = MUTAGEN_REPO_PATH) -> str:
    """
    Scans the Mutagen repository and rebuilds the vector index.
    Filters out auto-generated files.
    
    Args:
        repo_path: Path to the Mutagen src directory.
    """
    import time
    start_time = time.time()
    
    repo_path_obj = Path(repo_path)
    if not repo_path_obj.exists():
        return f"Error: Repository path does not exist: {repo_path}"

    logger.info(f"Scanning repository at: {repo_path}")
    
    # Load data
    reader = SimpleDirectoryReader(
        input_dir=str(repo_path_obj),
        recursive=True,
        required_exts=[".cs"],
        filename_as_id=True
    )
    
    try:
        all_docs = reader.load_data()
    except Exception as e:
        return f"Error loading data: {e}"
    
    logger.info(f"Loaded {len(all_docs)} documents. Filtering generated files...")
    
    # Filter out generated files
    filtered_docs = [
        d for d in all_docs 
        if not is_generated_file(d.metadata.get("file_path", ""))
    ]
    
    excluded_count = len(all_docs) - len(filtered_docs)
    logger.info(f"Filtered down to {len(filtered_docs)} documents. Excluded {excluded_count} generated files.")
    
    if not filtered_docs:
        return "Error: No documents remained after filtering. Check path and filter logic."

    # Add metadata
    for doc in filtered_docs:
        doc.metadata["source"] = "mutagen_handwritten"
        doc.metadata["indexed_at"] = str(Path(doc.metadata["file_path"]).stat().st_mtime)

    # Initialize ChromaDB Collection
    chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    # Create Index
    # insert_batch_size=SAFE_BATCH_SIZE to stay under ChromaDB limit
    index = VectorStoreIndex.from_documents(
        filtered_docs, 
        storage_context=storage_context,
        embed_model=embed_model,
        show_progress=True,
        insert_batch_size=SAFE_BATCH_SIZE
    )
    
    # Persist to disk (LlamaIndex metadata)
    index.storage_context.persist(persist_dir=STORAGE_PATH)
    
    elapsed = time.time() - start_time
    return f"âœ… Index refresh complete.\n- Time taken: {elapsed:.2f}s\n- Handwritten files registered: {len(filtered_docs)}\n- Excluded generated files: {excluded_count}\n- Storage path: {STORAGE_PATH}"

@mcp.tool()
def search_repository(query: str, top_k: int = 5) -> str:
    """
    Searches the Mutagen repository index for relevant code snippets.
    
    Args:
        query: The search query.
        top_k: Number of results to return.
    """
    try:
        # Check if collection exists
        try:
            chroma_collection = chroma_client.get_collection(COLLECTION_NAME)
        except ValueError:
             return "âŒ Index does not exist. Please run 'refresh_index' first."

        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            persist_dir=STORAGE_PATH
        )
        
        index = load_index_from_storage(
            storage_context,
            embed_model=embed_model
        )
        
        # Query Engine
        # Using "refine" as requested for better code retrieval accuracy
        query_engine = index.as_query_engine(
            similarity_top_k=top_k,
            response_mode="refine" 
        )
        response = query_engine.query(query)
        
        # Append source files
        sources = "\n".join([
            f"- {node.metadata.get('file_path', 'Unknown')}"
            for node in response.source_nodes
        ])
        
        return f"{response}\n\nðŸ“‚ Source Files:\n{sources}"
        
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return f"âŒ Error during search: {e}"

@mcp.tool()
def get_index_stats() -> str:
    """Returns statistics about the current index."""
    try:
        chroma_collection = chroma_client.get_collection(COLLECTION_NAME)
        count = chroma_collection.count()
        return f"ðŸ“Š Index Statistics\n- Total Documents: {count}\n- Collection Name: {COLLECTION_NAME}\n- Storage Path: {STORAGE_PATH}"
    except Exception as e:
        return f"Failed to get stats: {str(e)}"

if __name__ == "__main__":
    mcp.run()
