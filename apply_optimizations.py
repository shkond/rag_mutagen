#!/usr/bin/env python3
"""
Apply optimizations to server.py
"""

# Read the file
with open('server.py', 'r', encoding='utf-8') as f:
    lines = f.readlines()

# Find andpatch the necessary lines
output = []
i = 0
while i < len(lines):
    line = lines[i]
    
    # 1. Add CodeSplitter import after TextNode import
    if 'from llama_index.core.schema import TextNode' in line:
        output.append(line)
        output.append('from llama_index.core.node_parser import CodeSplitter\n')
        i += 1
        continue
    
    # 2. Replace Settings.chunk_size block with code_splitter
    if 'Settings.chunk_size = 2048' in line:
        # Skip next line too (Settings.chunk_overlap)
        output.append('# Code splitter for C# - chunk by semantic code blocks rather than tokens\n')
        output.append('code_splitter = CodeSplitter(\n')
        output.append('    language="c_sharp",\n')
        output.append('    chunk_lines=40,  # Roughly 40 lines per chunk\n')
        output.append('    chunk_lines_overlap=15,  # Overlap to maintain context\n')
        output.append('    max_chars=2048,  # Safety limit\n')
        output.append(')\n')
        i += 2  # Skip this line and next
        continue
    
    # 3. Add BM25 cache after COLLECTION_NAME
    if'COLLECTION_NAME = "mutagen_handwritten_code"' in line:
        output.append(line)
        output.append('\n')
        output.append('# BM25 Retriever Cache (cleared on refresh_index)\n')
        output.append('_CACHED_BM25_RETRIEVER = None\n')
        i += 1
        continue
    
    # 4. Replace is_generated_file function
    if 'def is_generated_file(file_path: str) -> bool:' in line:
        # Find the end of the function (return False line)
        func_start = i
        while i < len(lines) and not (lines[i].strip() == 'return False' and i > func_start + 5):
            i += 1
        i += 1  # Skip the return False line
        
        # Write new functions
        output.append('def is_generated_file_fast(file_path: Path) -> bool:\n')
        output.append('    """\n')
        output.append('    Fast path-based filtering without file I/O.\n')
        output.append('    Checks file extensions and directory names only.\n')
        output.append('    """\n')
        output.append('    path_str = str(file_path)\n')
        output.append('    \n')
        output.append('    # 1. File name patterns\n')
        output.append('    generated_suffixes = [".g.cs", ".Autogen.cs", ".Generated.cs"]\n')
        output.append('    if any(path_str.endswith(suffix) for suffix in generated_suffixes):\n')
        output.append('        return True\n')
        output.append('    \n')
        output.append('    # 2. Directory names\n')
        output.append('    excluded_dirs = ["Generated", "obj", "bin", ".vs", "Autogenerated"]\n')
        output.append('    if any(excluded_dir in file_path.parts for excluded_dir in excluded_dirs):\n')
        output.append('        return True\n')
        output.append('        \n')
        output.append('    return False\n')
        output.append('\n')
        output.append('def is_header_generated(text: str) -> bool:\n')
        output.append('    """\n')
        output.append('    Check if file content has auto-generated markers.\n')
        output.append('    This is the slow check that reads file content.\n')
        output.append('    """\n')
        output.append('    # Only check first 1000 chars for performance\n')
        output.append('    header = text[:1000]\n')
        output.append('    generated_markers = [\n')
        output.append('        "<auto-generated>",\n')
        output.append('        "This code was generated",\n')
        output.append('        "// <auto-generated />",\n')
        output.append('        "Auto-generated code"\n')
        output.append('    ]\n')
        output.append('    return any(marker in header for marker in generated_markers)\n')
        continue
    
    # 5. Replace refresh_index function body
    if '@mcp.tool()' in line and i + 1 < len(lines) and 'def refresh_index(' in lines[i+1]:
        output.append(line)  # @mcp.tool()
        i += 1
        output.append(lines[i])  # def refresh_index...
        i += 1
        # Skip docstring
        in_docstring = False
        if '"""' in lines[i]:
            in_docstring = True
            output.append(lines[i])
            i += 1
            while in_docstring and i < len(lines):
                output.append(lines[i])
                if '"""' in lines[i]:
                    in_docstring = False
                i += 1
        
        # Now write the new implementation
        output.append('    import time\n')
        output.append('    global _CACHED_BM25_RETRIEVER\n')
        output.append('    \n')
        output.append('    start_time = time.time()\n')
        output.append('    \n')
        output.append('    # Clear BM25 cache on refresh\n')
        output.append('    _CACHED_BM25_RETRIEVER = None\n')
        output.append('    logger.info("Cleared BM25 retriever cache")\n')
        output.append('    \n')
        output.append('    repo_path_obj = Path(repo_path)\n')
        output.append('    if not repo_path_obj.exists():\n')
        output.append('        return f"Error: Repository path does not exist: {repo_path}"\n')
        output.append('\n')
        output.append('    logger.info(f"Scanning repository at: {repo_path}")\n')
        output.append('    \n')
        output.append('    # PRE-FILTER FILES: Build filtered file list BEFORE loading into memory\n')
        output.append('    # This dramatically reduces memory usage and I/O for large codebases\n')
        output.append('    all_files = []\n')
        output.append('    excluded_dirs = {"obj", "bin", "Generated", ".vs", ".git", "Autogenerated"}\n')
        output.append('    \n')
        output.append('    for root, dirs, files in os.walk(str(repo_path_obj)):\n')
        output.append('        # Modify dirs in-place to skip excluded directories (prevents descending into them)\n')
        output.append('        dirs[:] = [d for d in dirs if d not in excluded_dirs]\n')
        output.append('        \n')
        output.append('        for file in files:\n')
        output.append('            if file.endswith(".cs"):\n')
        output.append('                full_path = Path(root) / file\n')
        output.append('                # Fast path-based filtering (no file I/O)\n')
        output.append('                if not is_generated_file_fast(full_path):\n')
        output.append('                    all_files.append(str(full_path))\n')
        output.append('    \n')
        output.append('    logger.info(f"Pre-filtered to {len(all_files)} .cs files before loading")\n')
        output.append('    \n')
        output.append('    if not all_files:\n')
        output.append('        return f"Error: No .cs files found after pre-filtering in {repo_path}"\n')
        output.append('    \n')
        output.append('    # Load only the pre-filtered files\n')
        output.append('    reader = SimpleDirectoryReader(\n')
        output.append('        input_files=all_files,\n')
        output.append('        filename_as_id=True\n')
        output.append('    )\n')
        output.append('    \n')
        output.append('    try:\n')
        output.append('        all_docs = reader.load_data()\n')
        output.append('    except Exception as e:\n')
        output.append('        return f"Error loading data: {e}"\n')
        output.append('    \n')
        output.append('    logger.info(f"Loaded {len(all_docs)} documents. Applying header-based filtering...")\n')
        output.append('    \n')
        output.append('    # Filter by content (header check) - only for docs that passed path filter\n')
        output.append('    filtered_docs = [\n')
        output.append('        d for d in all_docs \n')
        output.append('        if not is_header_generated(d.text)\n')
        output.append('    ]\n')
        output.append('    \n')
        output.append('    excluded_count = len(all_docs) - len(filtered_docs)\n')
        output.append('    logger.info(f"Filtered down to {len(filtered_docs)} documents. Excluded {excluded_count} files with generated headers.")\n')
        output.append('    \n')
        output.append('    if not filtered_docs:\n')
        output.append('        return "Error: No documents remained after filtering. Check path and filter logic."\n')
        output.append('\n')
        output.append('    # Add metadata\n')
        output.append('    for doc in filtered_docs:\n')
        output.append('        doc.metadata["source"] = "mutagen_handwritten"\n')
        output.append('        doc.metadata["indexed_at"] = str(Path(doc.metadata["file_path"]).stat().st_mtime)\n')
        output.append('        # Extract additional C# metadata\n')
        output.append('        try:\n')
        output.append('            doc.metadata.update(extract_metadata(doc.metadata["file_path"], doc.text))\n')
        output.append('        except Exception as e:\n')
        output.append('            logger.warning(f"Failed to extract metadata for {doc.metadata[\'file_path\']}: {e}")\n')
        output.append('\n')
        output.append('    # Initialize ChromaDB Collection\n')
        output.append('    chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)\n')
        output.append('    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n')
        output.append('    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n')
        output.append('    \n')
        output.append('    # Create Index with CodeSplitter for C#-aware chunking\n')
        output.append('    # insert_batch_size=SAFE_BATCH_SIZE to stay under ChromaDB limit\n')
        output.append('    index = VectorStoreIndex.from_documents(\n')
        output.append('        filtered_docs, \n')
        output.append('        storage_context=storage_context,\n')
        output.append('        embed_model=embed_model,\n')
        output.append('        transformations=[code_splitter],  # Use C# code-aware chunking\n')
        output.append('        show_progress=True,\n')
        output.append('        insert_batch_size=SAFE_BATCH_SIZE\n')
        output.append('    )\n')
        output.append('    \n')
        output.append('    # Persist to disk (LlamaIndex metadata)\n')
        output.append('    index.storage_context.persist(persist_dir=STORAGE_PATH)\n')
        output.append('    \n')
        output.append('    elapsed = time.time() - start_time\n')
        output.append('    return f"âœ… Index refresh complete.\\n- Time taken: {elapsed:.2f}s\\n- Handwritten files registered: {len(filtered_docs)}\\n- Excluded generated files: {len(all_files) - len(filtered_docs) + excluded_count}\\n- Storage path: {STORAGE_PATH}"\n')
        
        # Skip old implementation until next @mcp.tool()
        while i < len(lines) and '@mcp.tool()' not in lines[i]:
            i += 1
        continue
    
    output.append(line)
    i += 1

# Write the output
with open('server.py', 'w', encoding='utf-8') as f:
    f.writelines(output)

print("Optimizations applied successfully!")
